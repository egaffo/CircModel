---
title: "Update Summarized Benchmarks"
author: "Enrico Gaffo"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: hide
params:
  outdir: "DM1_updated_benchmark_results"
  inputData: "./DM1_benchmark_results/sumBenchs.qs"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(SummarizedBenchmark)
# library("magrittr")
library(data.table)
library(qs)
library(DT)

library(BiocParallel)
library(batchtools)

# library("limma")
# library("edgeR")
# library("DESeq2")
# library("tximport")
# library(glmGamPoi)
# library(zinbwave)
# library(scran)

# install.packages("devtools", Ncpus = multicoreWorkers())
# library(devtools)
# install_github("lichen-lab/circMeta", Ncpus = multicoreWorkers())
# library(circMeta)
# library(lncDIFF)
# library(ShrinkBayes)
# library(samr)
```

```{r}
## set the output directory
# outdir <- "benchmark_results"
outdir <- params$outdir
dir.create(path = outdir, recursive = T, showWarnings = F)
```

```{r}
nWorkers <- multicoreWorkers()
# nWorkers <- 12

prev_output <- file.path(outdir, "updated_sumBenchs.qs")
sumBench_perf_metrics_qs <- prev_output
if(file.exists(prev_output)){
    inputData <- prev_output
}else{
    ## load previous benchmark
    inputData <- params$inputData
}
sbL <- qread(file = inputData, nthreads = nWorkers)

# countData <- sbL$N03_bulk_de_01@BenchDesign@data@data$cntdat
# group <- sbL$N03_bulk_de_01@BenchDesign@data@data$coldat$condition
```

```{r}
# subsetIdx <- c(1:2, 31:32, 61:62, 91:92, 121:122, 151:152)
# subsetIdx <- 1:2
# subsetIdx <- 1:length(sbL) # all
# sbL <- sbL[subsetIdx]
sbL <- sbL[grep("bulk", names(sbL))]
```

```{r}
## set scheduler for cluster computing
# ncpus <- 1
# bpparam <- BatchtoolsParam(workers = length(sbL),
#                            saveregistry = F,
#                            cluster = "slurm",
#                            resources = list(ncpus = ncpus,
#                                             walltime = 3600, # 1h max
#                                             memory = 2048) # 4Gb, 2048 2GByte, 8192 8Gb
# )

## Multithread, single machine
bpparam <- MulticoreParam(4)
```

# Update bench design

## lncDIFF

```{r}
lncdiff_run <- function(countData, group) {
  
  tictoc::tic()

  dgeList <- edgeR::DGEList(counts = countData)
  # calculate normalization factors
  dgeList <- edgeR::calcNormFactors(object = dgeList, method = "TMM")
  ## normalize counts
  norm_data <- edgeR::cpm(dgeList, 
                          normalized.lib.sizes = TRUE, 
                          log = F)
  
  res <- lncDIFF::lncDIFF(edata = norm_data, 
                          group = as.character(group), 
                          covariate = NULL, 
                          link.function = 'log', 
                          CompareGroups = unique(as.character(group)),
                          simulated.pvalue = FALSE, 
                          permutation = 100)
  
  runtime <- tictoc::toc(log = F, quiet = T)
  
  list(res = res, 
       runtime = runtime$toc - runtime$tic)
  
}

lncdiff_pv <- function(x) {
    res <- x$res$DE.results$Pvalue
    res[is.na(res)] <- 1
    res
}

lncdiff_apv <- function(x) {
  res <- p.adjust(x$res$DE.results$Pvalue, method = "BH")
  res[is.na(res)] <- 1
  res
}

lncdiff_lfc <- function(x) {
  x$res$DE.results$Log2.Fold.Change
}

lncdiff_time <- function(x) { rep(as.numeric(x$runtime), length(x$res$DE.results$Pvalue)) }
```

### Update the Build bench list

```{r}
# https://www.bioconductor.org/packages/devel/bioc/vignettes/SummarizedBenchmark/inst/doc/Feature-Iterative.html#using-updatebench
## override BPPARAM
# parallel_methods <- 4
# bpparam$resources$ncpus <- parallel_methods

func_list <- list(lncdiff_run = lncdiff_run,
                  lncdiff_pv = lncdiff_pv,
                  lncdiff_apv = lncdiff_apv,
                  lncdiff_lfc = lncdiff_lfc,
                  lncdiff_time = lncdiff_time)

sbL <- 
    bplapply(sbL, 
             function(sb,
                      func_list) { 
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "lncDIFF_Dt_LRT", 
                                                      func = func_list$lncdiff_run, 
                                                      post = list(pv = func_list$lncdiff_pv,
                                                                  adj_pv = func_list$lncdiff_apv,
                                                                  lfc = func_list$lncdiff_lfc,
                                                                  runtime = func_list$lncdiff_time),
                                                      meta = list(pkg_name = "lncDIFF", 
                                                                  pkg_vers = as.character(packageVersion("lncDIFF"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             }, 
             func_list = func_list,
             BPPARAM = bpparam)
# sbL
```

```{r}
## error handling
show_dt <- 
  dcast(melt(rbindlist(lapply(sbL, 
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)), 
                                                    keep.rownames = "Assay")), 
                       idcol = "DS"), 
             id.vars = c("DS", "Assay"), 
             variable.name = "Method"), 
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" | 
                    lfc != "success" | 
                    pv != "success" | 
                    runtime != "success"], 
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

## SAMseq

```{r}
samseq_run <- function(countData, group) {
  
  library(samr)
  
  tictoc::tic()
  
  condition.12 <- rep(1, length(group))
  condition.12[which(group == levels(factor(group))[2])] <- 2
  
  SAMseq.test <- samr::SAMseq(countData, 
                              condition.12, 
                              resp.type = 'Two class unpaired', 
                              geneid = rownames(countData), 
                              genenames = rownames(countData), 
                              nperms = 100, 
                              nresamp = 20,
                              fdr.output = 1)
  pv <- samr.pvalues.from.perms(tt = SAMseq.test$samr.obj$tt, 
                                ttstar = SAMseq.test$samr.obj$ttstar)
  
  SAMseq.result <- rbind(SAMseq.test$siggenes.table$genes.up,
                         SAMseq.test$siggenes.table$genes.lo)
  
  SAMseq.statistic <- rep(0, nrow(countData))
  
  SAMseq.statistic[match(SAMseq.result[, 1], 
                         rownames(countData))] <- as.numeric(SAMseq.result[, 3])
  
  SAMseq.FDR <- rep(1, nrow(countData))
  
  SAMseq.FDR[match(SAMseq.result[, 1], 
                   rownames(countData))] <- as.numeric(SAMseq.result[, 5]) / 100
  
  SAMseq.FC <- rep(NA, nrow(countData))
  
  SAMseq.FC[match(SAMseq.result[, 1], 
                   rownames(countData))] <- as.numeric(SAMseq.result[, 4])
  
  SAMseq.score <- 1 - SAMseq.FDR
  
  res <- data.frame('statistic' = SAMseq.statistic, 
                    'Pvalue' = pv,
                    'FDR' = SAMseq.FDR, 
                    'FC' = SAMseq.FC,
                    'score' = SAMseq.score)
  
  runtime <- tictoc::toc(log = F, quiet = T)
  
  list(res = res, 
       runtime = runtime$toc - runtime$tic)
}

samseq_pv <- function(x) {
  res <- x$res$Pvalue
  res[is.na(res)] <- 1
  res
}


samseq_apv <- function(x) {
  # p.adjust(x$res$Pvalue, "BH")
  res <- x$res$FDR
  res[is.na(res)] <- 1
  res
}

samseq_lfc <- function(x) {
  log2(x$res$FC)
}

samseq_time <- function(x) { rep(as.numeric(x$runtime), nrow(x$res)) }
```

### Update the Build bench list

```{r}
# https://www.bioconductor.org/packages/devel/bioc/vignettes/SummarizedBenchmark/inst/doc/Feature-Iterative.html#using-updatebench
## override BPPARAM
# parallel_methods <- 4
# bpparam$resources$ncpus <- parallel_methods

func_list <- list(samseq_run = samseq_run,
                  samseq_pv = samseq_pv,
                  samseq_apv = samseq_apv,
                  samseq_lfc = samseq_lfc,
                  samseq_time = samseq_time)

sbL <- 
    bplapply(sbL, 
             function(sb,
                      # parallel_methods, 
                      func_list) { 
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "SAMseq_Dt_TT", 
                                                      func = func_list$samseq_run,
                                                      post = list(pv = func_list$samseq_pv,
                                                                  adj_pv = func_list$samseq_apv,
                                                                  lfc = func_list$samseq_lfc,
                                                                  runtime = func_list$samseq_time),
                                                      meta = list(pkg_name = "samr", 
                                                                  pkg_vers = as.character(packageVersion("samr"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             }, 
             func_list = func_list,
             # parallel_methods = parallel_methods,
             BPPARAM = bpparam)
# sbL
```

```{r}
## error handling
show_dt <- 
  dcast(melt(rbindlist(lapply(sbL, 
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)), 
                                                    keep.rownames = "Assay")), 
                       idcol = "DS"), 
             id.vars = c("DS", "Assay"), 
             variable.name = "Method"), 
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" | 
                    lfc != "success" | 
                    pv != "success" | 
                    runtime != "success"], 
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
## save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## NBID

```{r}
nbid_run <- function(countData, group) {
    
    tictoc::tic()
    
    res <- NBID::DEUsingNBID(countData, group, ncore = 1)
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = res, 
         runtime = runtime$toc - runtime$tic)
}

## use NBID with scran size factors
nbid_sc_run <- function(countData, group) {
    
    tictoc::tic()
    
    # sf <- scran::calculateSumFactors(countData, clusters = group)
    sf <- scuttle::pooledSizeFactors(countData, clusters = group)
    # sf <- scuttle::pooledSizeFactors(countData) # this reduces to librarySizeFactors(countData)
    res <- NBID::DEUsingNBID(countData, group, ncore = 1, countPerCell = sf)
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = res, 
         runtime = runtime$toc - runtime$tic)
}

nbid_pv <- function(x) {
    res <- x$res[, "pvalue"]
    res[is.na(res)] <- 1
    res
}

nbid_apv <- function(x) {
    res <- p.adjust(x$res[, "pvalue"], method = "BH")
    res[is.na(res)] <- 1
    res
}

nbid_lfc <- function(x) {
    x$res[, "log2FC2"]
}

nbid_time <- function(x) {
    rep(as.numeric(x$runtime), length(x$res[, "pvalue"]))
}

# res <- NBID::DEAnalysisSCUsingEdgeR(smallData$count, smallData$groupLabel)
```

### Update the Build bench list

#### NBID 

```{r}
# https://www.bioconductor.org/packages/devel/bioc/vignettes/SummarizedBenchmark/inst/doc/Feature-Iterative.html#using-updatebench
## override BPPARAM
# parallel_methods <- 4
# bpparam$resources$ncpus <- parallel_methods

func_list <- list(nbid_run = nbid_run,
                  nbid_pv = nbid_pv,
                  nbid_apv = nbid_apv,
                  nbid_lfc = nbid_lfc,
                  nbid_time = nbid_time)

sbL <- 
    bplapply(sbL, 
             function(sb,
                      # parallel_methods, 
                      func_list) { 
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "NBID_Dt_LRT", 
                                                      func = func_list$nbid_run,
                                                      post = list(pv = func_list$nbid_pv,
                                                                  adj_pv = func_list$nbid_apv,
                                                                  lfc = func_list$nbid_lfc,
                                                                  runtime = func_list$nbid_time),
                                                      meta = list(pkg_name = "NBID", 
                                                                  pkg_vers = as.character(packageVersion("NBID"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             }, 
             func_list = func_list,
             # parallel_methods = parallel_methods,
             BPPARAM = bpparam)
```

#### NBID scran

```{r}
func_list <- list(nbid_run = nbid_sc_run,
                  nbid_pv = nbid_pv,
                  nbid_apv = nbid_apv,
                  nbid_lfc = nbid_lfc,
                  nbid_time = nbid_time)

sbL <- 
    bplapply(sbL, 
             function(sb,
                      # parallel_methods, 
                      func_list) { 
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "NBID_Sc_LRT", 
                                                      func = func_list$nbid_run,
                                                      post = list(pv = func_list$nbid_pv,
                                                                  adj_pv = func_list$nbid_apv,
                                                                  lfc = func_list$nbid_lfc,
                                                                  runtime = func_list$nbid_time),
                                                      meta = list(pkg_name = "NBID", 
                                                                  pkg_vers = as.character(packageVersion("NBID"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             }, 
             func_list = func_list,
             # parallel_methods = parallel_methods,
             BPPARAM = bpparam)
# sbL
```

```{r}
## error handling
show_dt <- 
  dcast(melt(rbindlist(lapply(sbL, 
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)), 
                                                    keep.rownames = "Assay")), 
                       idcol = "DS"), 
             id.vars = c("DS", "Assay"), 
             variable.name = "Method"), 
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" | 
                    lfc != "success" | 
                    pv != "success" | 
                    runtime != "success"], 
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
## save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## NOISeqBIO

```{r}
noiseqbio_run <- function(countData, group) {
    
    tictoc::tic()
    
    # mynoiseq <- NOISeq::noiseq(countData, 
    #                            k = 0.5, 
    #                            norm = "rpkm", 
    #                            factor = "Tissue", 
    #                            pnr = 0.2, 
    #                            nss = 5, 
    #                            v = 0.02, 
    #                            lc = 1, 
    #                            replicates = "technical")
    # head(mynoiseq@results[[1]])
    # 
    # mynoiseq.tmm <- NOISeq::noiseq(countData, 
    #                                k = 0.5, 
    #                                norm = "tmm", 
    #                                factor = "TissueRun",
    #                                conditions = c("Kidney_1", "Liver_1"), 
    #                                lc = 0, 
    #                                replicates = "technical")
    
    res <- NOISeq::noiseqbio(NOISeq::readData(countData, group), 
                             k = 0.5, 
                             norm = "tmm", 
                             factor = "condition",
                             lc = 0,
                             # r = 20, 
                             adj = 1.5, 
                             plot = FALSE, 
                             a0per = 0.9, 
                             random.seed = 12345,
                             filter = 0) #disabled filter
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = res, 
         runtime = runtime$toc - runtime$tic)
}

noiseqbio_pv <- function(x) {
    
    ## NB: NOISeq does not provide P-values, but only "adjusted p-value"-like probabilities
    res <- 1 - x$res@results[[1]]$prob
    res[is.na(res)] <- 1
    res
}

noiseqbio_apv <- function(x) {
    
    # res@results[[1]]$prob gives the estimated probability of differential 
    # expression for each feature. Note that when using NOISeq, these probabilities 
    # are not equivalent to p-values. The higher the probability, the more likely 
    # that the difference in expression is due to the change in the experimental 
    # condition and not to chance.
    # when using NOISeqBIO, the probability of differential expression would be 
    # equivalent to 1 − FDR, where FDR can be considered as an adjusted p-value. 
    # Hence, in this case, it would be more convenient to use q = 0.95.
    
    res <- 1 - x$res@results[[1]]$prob
    res[is.na(res)] <- 1
    res
}

noiseqbio_lfc <- function(x) {
    x$res@results[[1]]$log2FC
}

noiseqbio_time <- function(x) {
    rep(as.numeric(x$runtime), length(x$res@results[[1]]$log2FC))
}
```

### Update the Build bench list

```{r}
func_list <- list(run = noiseqbio_run,
                  pv = noiseqbio_pv,
                  apv = noiseqbio_apv,
                  lfc = noiseqbio_lfc,
                  time = noiseqbio_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      # parallel_methods,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)

                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "NOISeqBIO_TMM_LRT",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "NOISeqBIO",
                                                                  pkg_vers = as.character(packageVersion("NOISeq"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat))

                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             # parallel_methods = parallel_methods,
             BPPARAM = bpparam)
# sbL
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## PoissonSeq

```{r}
poissonSeq_run <- function(countData, group) {
    
    # library(PoissonSeq)
    library(splines)
    tictoc::tic()
    
    dat <- list(n = countData,
                y = as.numeric(group),
                pair = FALSE,
                type = 'twoclass',
                gname = rownames(countData))
    
    res <- PoissonSeq::PS.Main(dat = dat, para = list(ct.sum = 2,
                                                      ct.mean = 0.1))
    res <- res[rownames(countData), ]
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = res, 
         runtime = runtime$toc - runtime$tic)
}

poissonSeq_pv <- function(x) {
    res <- x$res$pval
    res[is.na(res)] <- 1
    res
}

poissonSeq_apv <- function(x) {
    res <- x$res$fdr
    res[is.na(res)] <- 1
    res
}

poissonSeq_lfc <- function(x) {
    x$res$log.fc
}

poissonSeq_time <- function(x) { 
    rep(as.numeric(x$runtime), length(x$res$pval))
}
```

### Update the Build bench list

```{r}
func_list <- list(run = poissonSeq_run,
                  pv = poissonSeq_pv,
                  apv = poissonSeq_apv,
                  lfc = poissonSeq_lfc,
                  time = poissonSeq_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)

                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "PoissonSeq_Dt_TT",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "PoissonSeq",
                                                                  pkg_vers = as.character(packageVersion("PoissonSeq"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))

                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             BPPARAM = bpparam)
# sbL
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## edgeR TMMwsp

```{r}
##  robust
edgeRrbstTMMswp_run <- function(countData, group, design) {
  
  tictoc::tic()
  
  y <- edgeR::DGEList(countData, group = group)
  y <- edgeR::calcNormFactors(y, method = "TMMwsp")
  des <- model.matrix(~group, data = y$samples)
  y <- edgeR::estimateGLMRobustDisp(y, des) 
  fit <- edgeR::glmQLFit(y = y, 
                         dispersion = y$tagwise.dispersion, 
                         robust = TRUE, 
                         design = des)
  res <- edgeR::glmLRT(fit, coef = 2)
  
  runtime <- tictoc::toc(log = F, quiet = T)
  
  list(res = res, 
       runtime = runtime$toc - runtime$tic)
}

## post functions
edgeR_pv <- function(x) { 
  res <- x$res$table$PValue 
  res[is.na(res)] <- 1
  res
}

edgeR_apv <- function(x) {
  res <- p.adjust(p = x$res$table$PValue, method = "BH")
  # res <- topTags(x, number = Inf, sort.by = "none")$table$FDR
  res[is.na(res)] <- 1
  res
}

edgeR_lfc <- function(x) { x$res$table$logFC }

edgeR_time <- function(x) { rep(as.numeric(x$runtime), nrow(x$res)) }
```

### Update the Build bench list

```{r}
func_list <- list(run = edgeRrbstTMMswp_run,
                  pv = edgeR_pv,
                  apv = edgeR_apv,
                  lfc = edgeR_lfc,
                  time = edgeR_time)

sbL <-
    bplapply(sbL, function(sb, func_list) {
        bd <- SummarizedBenchmark::BenchDesign(sb)
        
        bd <- SummarizedBenchmark::addMethod(bd = bd,
                                             label = "edgeR_rbstTMMswp_LRT",
                                             func = func_list$run,
                                             post = list(pv = func_list$pv,
                                                         adj_pv = func_list$apv,
                                                         lfc = func_list$lfc,
                                                         runtime = func_list$time),
                                             meta = list(pkg_name = "edgeR", 
                                                         pkg_vers = as.character(packageVersion("edgeR"))),
                                             params = rlang::quos(countData = cntdat,
                                                                  group = coldat$condition,
                                                                  design = ~coldat$condition))
        
        SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
    },
    func_list = func_list,
    BPPARAM = bpparam)
# sbL
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## Limma + sctransform::vst

```{r}
limmaStVst_run <- function(countData, group) {

    tictoc::tic()

    sampleTab <- data.frame(condition = setNames(group, colnames(countData)))
    des <- model.matrix(~ condition, data = sampleTab)
    
    set.seed(12345)
    vst_out <- sctransform::vst(countData,
                                cell_attr = sampleTab,
                                latent_var = c("condition"), #c("log_umi"),
                                method = "glmGamPoi",
                                # method = "glmGamPoi_offset",
                                # method = "offset",
                                return_gene_attr = TRUE,
                                return_cell_attr = TRUE,
                                min_cells = 3,
                                n_genes = NULL,
                                verbosity = 1)

    fit <- limma::eBayes(limma::lmFit(vst_out$y, des))
    
    res <- limma::topTable(fit, 
                           number = nrow(countData))[rownames(countData), ]
    
    runtime <- tictoc::toc(log = F, quiet = T)

    list(res = res,
         runtime = runtime$toc - runtime$tic)
}

limmaStVst_pv <- function(x) {
    res <- x$res$P.Value
    res[is.na(res)] <- 1
    res
}

limmaStVst_apv <- function(x) {
    # res <- p.adjust(p = x$p.value[, 2], method = "BH")
    res <- x$res$adj.P.Val
    res[is.na(res)] <- 1
    res
}

limmaStVst_lfc <- function(x) {
    x$res$logFC
}

limmaStVst_time <- function(x) { rep(as.numeric(x$runtime), nrow(x$res)) }
```

### Update the Build bench list

```{r}
func_list <- list(run = limmaStVst_run,
                  pv = limmaStVst_pv,
                  apv = limmaStVst_apv,
                  lfc = limmaStVst_lfc,
                  time = limmaStVst_time)

sbL <-
    bplapply(sbL, function(sb, func_list) {
        bd <- SummarizedBenchmark::BenchDesign(sb)
        
        bd <- SummarizedBenchmark::addMethod(bd = bd,
                                             label = "limma_St_Vst",
                                             func = func_list$run,
                                             post = list(pv = func_list$pv,
                                                         adj_pv = func_list$apv,
                                                         lfc = func_list$lfc,
                                                         runtime = func_list$time),
                                             meta = list(pkg_name = "LimmaVst",
                                                         pkg_vers = paste(as.character(packageVersion("limma")),
                                                                          as.character(packageVersion("sctransform")),
                                                                          sep = "/")),
                                             params = rlang::quos(countData = cntdat,
                                                                  group = coldat$condition))
        
        SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
    },
    func_list = func_list,
    BPPARAM = bpparam)
# sbL
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## ROTS

```{r}
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_ROTScpm.R
ROTScpm_run <- function(countData, group) {
    
    tictoc::tic()
    
    grp <- as.character(group)
    dge <- edgeR::DGEList(counts = countData)
    dge <- edgeR::calcNormFactors(dge, method = "TMM")
    cpms <- edgeR::cpm(dge)
    rots <- ROTS::ROTS(data = cpms,
                       groups = grp,
                       B = 1000,
                       K = 1000,
                       log = FALSE,
                       seed = 123)
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = rots, 
         runtime = runtime$toc - runtime$tic)
}

ROTScpm_pv <- function(x) {
    res <- x$res$pvalue
    res[is.na(res)] <- 1
    res
}

ROTScpm_apv <- function(x) {
    # res <- p.adjust(p = x$res$pvalue, method = "BH")
    res <- x$res$FDR
    res[is.na(res)] <- 1
    res
}

ROTScpm_lfc <- function(x) {
    x$res$logfc
}

ROTScpm_time <- function(x) { rep(as.numeric(x$runtime), nrow(x$res$data)) }

# run_ROTSvoom <- function(L) {
#     grp <- L$condt
#     dge <- DGEList(counts = L$count)
#     dge <- edgeR::calcNormFactors(dge)
#     vm <- voom(dge, design = model.matrix(~ grp))
#     rots <- ROTS(data = vm$E, groups = grp, B = 1000, K = 1000, log = TRUE, seed = 123)
#   list(session_info = session_info,
#        timing = timing,
#        res = rots,
#        df = data.frame(pval = rots$pvalue,
#                        row.names = rownames(rots$data)))
# }

# ROTS <- function(countData, group, design) {
#     library(ROTS)
#     log=F
#     transformation<-T
#     normalize<-T
#     if(normalize&&transformation){
#     if(transformation){
#       log=TRUE
#       nf <- edgeR::calcNormFactors(count.matrix(cdata), method = 'TMM' )
#       voom.data <- limma::voom(count.matrix(cdata),
#                                design = model.matrix(~factor(sample.annotations(cdata)$condition)),
#                                lib.size = colSums(count.matrix(cdata)) * nf)
#       Exp <- voom.data$E
#     }else{
#       edgeR.dgelist <- edgeR::DGEList(counts = count.matrix(cdata),
#                                       group = factor(sample.annotations(cdata)$condition))
#       edgeR.dgelist <- edgeR::calcNormFactors(edgeR.dgelist, method = 'TMM')
#       Factors <- edgeR.dgelist$samples$lib.size * edgeR.dgelist$samples$norm.factors
#       Exp <- t(t(edgeR.dgelist$counts)/Factors) * mean(Factors)
#     }
#   }else{
#     Exp <- count.matrix(cdata)
#   }
# 
# 
#   results <- ROTS::ROTS(data = Exp,
#                         groups = as.numeric(as.character(factor(sample.annotations(cdata)$condition))),
#                         K = NULL, B = 1000, log = log)
#   rots.pvalues <- results$pvalue
#   rots.logFC <- results$logfc
#   rots.FDR <- results$FDR
#   result.table <- data.frame('pvalue' = rots.pvalues, 'logFC' = rots.logFC, 'FDR' = rots.FDR)
# }
```

### Update the Build bench list

```{r}
func_list <- list(run = ROTScpm_run,
                  pv = ROTScpm_pv,
                  apv = ROTScpm_apv,
                  lfc = ROTScpm_lfc,
                  time = ROTScpm_time)

sbL <-
    bplapply(sbL, function(sb, func_list) {
        bd <- SummarizedBenchmark::BenchDesign(sb)
        
        bd <- SummarizedBenchmark::addMethod(bd = bd,
                                             label = "ROTScpm_Dt_TLT", ## t-like test statistics
                                             func = func_list$run,
                                             post = list(pv = func_list$pv,
                                                         adj_pv = func_list$apv,
                                                         lfc = func_list$lfc,
                                                         runtime = func_list$time),
                                             meta = list(pkg_name = "ROTScpm",
                                                         pkg_vers = paste(as.character(packageVersion("ROTS")),
                                                                          as.character(packageVersion("edgeR")),
                                                                          sep = "/")),
                                             params = rlang::quos(countData = cntdat,
                                                                  group = coldat$condition))
        
        SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
    },
    func_list = func_list,
    BPPARAM = bpparam)
# sbL
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## DESingle

```{r}
DEsingle_run <- function(countData, group) {
    
    tictoc::tic()
    
    results <- DEsingle::DEsingle(counts = countData, group = group)
    results <- data.frame(results, stringsAsFactors = FALSE)
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = results, 
         runtime = runtime$toc - runtime$tic)
    
}

DEsingle_pv <- function(x) {
    res <- x$res$pvalue
    res[is.na(res)] <- 1
    res
}

DEsingle_apv <- function(x) {
    # res <- p.adjust(p = x$res$pvalue, method = "BH")
    res <- x$res$pvalue.adj.FDR
    res[is.na(res)] <- 1
    res
}

DEsingle_lfc <- function(x) {
    log2(x$res$foldChange)
}

DEsingle_time <- function(x) { rep(as.numeric(x$runtime), nrow(x$res)) }
```

### Update the Build bench list

```{r}
func_list <- list(run = DEsingle_run,
                  pv = DEsingle_pv,
                  apv = DEsingle_apv,
                  lfc = DEsingle_lfc,
                  time = DEsingle_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)

                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "DEsingle_Dt_LRT",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "DEsingle",
                                                                  pkg_vers = as.character(packageVersion("DEsingle"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))

                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             BPPARAM = bpparam)
# sbL
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## metagenomeSeq

```{r}
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_metagenomeSeq.R
metagenomeSeq_run <- function(countData, group) {
    
    tictoc::tic()
    
    sampleTable <- data.frame(condition = group, 
                              row.names = colnames(countData))
    
    obj <- metagenomeSeq::newMRexperiment(countData, 
                                          phenoData = new("AnnotatedDataFrame", 
                                                          data = sampleTable))
    p <- metagenomeSeq::cumNormStatFast(obj)
    obj <- metagenomeSeq::cumNorm(obj, p = p)
    mod <- model.matrix(~ condition, data = sampleTable) # data = Biobase::pData(obj)
    res <- metagenomeSeq::fitFeatureModel(obj = obj, mod = mod, coef = 2)
    tbl <- metagenomeSeq::MRtable(obj = res, number = Inf, by = 2, 
                                  adjustMethod = "BH")[rownames(countData), ]
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = tbl, 
         runtime = runtime$toc - runtime$tic)
}

metagenomeSeq_pv <- function(x) {
    res <- x$res$pvalues
    res[is.na(res)] <- 1
    res
}

metagenomeSeq_apv <- function(x) {
    res <- x$res$adjPvalues
    res[is.na(res)] <- 1
    res
}

metagenomeSeq_lfc <- function(x) {
    x$res$logFC
}

metagenomeSeq_time <- function(x) { rep(as.numeric(x$runtime), length(x$res$pvalues)) }
```

### Update the Build bench list

```{r}
func_list <- list(run = metagenomeSeq_run,
                  pv = metagenomeSeq_pv,
                  apv = metagenomeSeq_apv,
                  lfc = metagenomeSeq_lfc,
                  time = metagenomeSeq_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "metagenomeSeq_Dt_EBT",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "metagenomeSeq",
                                                                  pkg_vers = as.character(packageVersion("metagenomeSeq"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             BPPARAM = bpparam)
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## Seurat Bimod

```{r}
## Seurat (Bimod)
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_SeuratTobit.R
seuratBimod_run <- function(countData, group) {
    
    tictoc::tic()
    
    colnames(countData) <- paste0(group, "__", 1:ncol(countData))
    ##CreateSeuratObject::CreateSeuratObject()
    seur <- Seurat::CreateSeuratObject(counts = countData, 
                                       project = "scrnaseq", 
                                       assay = "RNA", 
                                       names.field = 1, 
                                       names.delim = "__")
    res <- Seurat::FindMarkers(seur, 
                               min.pct = 0,
                               verbose = F, 
                               ident.1 = levels(factor(group))[1],
                               ident.2 = levels(factor(group))[2], 
                               test.use = "bimod") #"wilcox" "MAST"
    
    ## FindMarkers may return less features than the input
    ## the following fixes the missing features
    missed_genes <- rownames(countData)[which(!sub("_", "-", rownames(countData)) %in% rownames(res))]
    
    nadf <- data.frame("p_val" = rep(NA, length(missed_genes)), 
                       "avg_log2FC" = rep(NA, length(missed_genes)), 
                       "pct.1" = rep(NA, length(missed_genes)),  
                       "pct.2" = rep(NA, length(missed_genes)), 
                       "p_val_adj" = rep(NA, length(missed_genes)), 
                       row.names = sub("_", "-", missed_genes))
    
    ## attach missed rows and order according to initial names
    res <- rbind(res, nadf)[sub("_", "-", rownames(countData)), ]
    rownames(res) <- rownames(countData)
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = res, 
         runtime = runtime$toc - runtime$tic)
}

seuratBimod_pv <- function(x) {
    res <- x$res$p_val
    res[is.na(res)] <- 1
    res
}

seuratBimod_apv <- function(x) {
    res <- x$res$p_val_adj
    res[is.na(res)] <- 1
    res
}

seuratBimod_lfc <- function(x) {
    x$res$avg_log2FC
}

seuratBimod_time <- function(x) { rep(as.numeric(x$runtime), length(x$res$p_val)) }
```

### Update the Build bench list

```{r}
func_list <- list(run = seuratBimod_run,
                  pv = seuratBimod_pv,
                  apv = seuratBimod_apv,
                  lfc = seuratBimod_lfc,
                  time = seuratBimod_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "seurat_Bim_LRT",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "Seurat",
                                                                  pkg_vers = as.character(packageVersion("Seurat"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             BPPARAM = bpparam)
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## Seurat Wilcoxon

```{r}
## Seurat (Bimod)
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_SeuratTobit.R
seuratWilcox_run <- function(countData, group) {
    
    tictoc::tic()
    
    colnames(countData) <- paste0(group, "__", 1:ncol(countData))
    ##CreateSeuratObject::CreateSeuratObject()
    seur <- Seurat::CreateSeuratObject(counts = countData, 
                                       project = "scrnaseq", 
                                       assay = "RNA", 
                                       names.field = 1, 
                                       names.delim = "__")
    res <- Seurat::FindMarkers(seur, 
                               min.pct = 0,
                               verbose = F, 
                               ident.1 = levels(factor(group))[1],
                               ident.2 = levels(factor(group))[2], 
                               test.use = "wilcox")
    
    ## FindMarkers may return less features than the input
    ## the following fixes the missing features
    missed_genes <- rownames(countData)[which(!sub("_", "-", rownames(countData)) %in% rownames(res))]
    
    nadf <- data.frame("p_val" = rep(NA, length(missed_genes)), 
                       "avg_log2FC" = rep(NA, length(missed_genes)), 
                       "pct.1" = rep(NA, length(missed_genes)),  
                       "pct.2" = rep(NA, length(missed_genes)), 
                       "p_val_adj" = rep(NA, length(missed_genes)), 
                       row.names = sub("_", "-", missed_genes))
    
    ## attach missed rows and order according to initial names
    res <- rbind(res, nadf)[sub("_", "-", rownames(countData)), ]
    rownames(res) <- rownames(countData)
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = res, 
         runtime = runtime$toc - runtime$tic)
}

seuratWilcox_pv <- function(x) {
    res <- x$res$p_val
    res[is.na(res)] <- 1
    res
}

seuratWilcox_apv <- function(x) {
    res <- x$res$p_val_adj
    res[is.na(res)] <- 1
    res
}

seuratWilcox_lfc <- function(x) {
    x$res$avg_log2FC
}

seuratWilcox_time <- function(x) { rep(as.numeric(x$runtime), length(x$res$p_val)) }
```

### Update the Build bench list

```{r}
func_list <- list(run = seuratWilcox_run,
                  pv = seuratWilcox_pv,
                  apv = seuratWilcox_apv,
                  lfc = seuratWilcox_lfc,
                  time = seuratWilcox_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "seurat_Dt_WLX",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "Seurat",
                                                                  pkg_vers = as.character(packageVersion("Seurat"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             BPPARAM = bpparam)
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## MAST

```{r}
# MAST (on CPM counts)
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_MASTcpm.R
MASTcpm_run <- function(countData, group) {
  
    tictoc::tic()
    
    grp <- group
    dge <- edgeR::DGEList(counts = countData)
    dge <- edgeR::calcNormFactors(dge, method = "TMM")
    # cpms <- edgeR::cpm(dge)
    lgcpms <- edgeR::cpm(dge, log = T)
    sca <- MAST::FromMatrix(exprsArray = lgcpms, #log2(cpms + 1),
                            cData = data.frame(wellKey = colnames(lgcpms),
                                               grp = grp))
    # zlmdata <- MAST::zlm.SingleCellAssay(~grp, sca)
    zlmdata <- MAST::zlm(~ grp, sca)
    mast <- MAST::lrTest(zlmdata, "grp")
    
    lfc <- MAST::getLogFC(zlmdata)
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = list(mast = mast, lfc = lfc), 
         runtime = runtime$toc - runtime$tic)
}

MASTcpm_pv <- function(x) {
    res <- x$res$mast[, "hurdle", "Pr(>Chisq)"]
    res[is.na(res)] <- 1
    res
}

MASTcpm_apv <- function(x) {
    res <- p.adjust(x$res$mast[, "hurdle", "Pr(>Chisq)"], method = "BH")
    res[is.na(res)] <- 1
    res
}

MASTcpm_lfc <- function(x) {
    x$res$lfc$logFC
}

MASTcpm_time <- function(x) { rep(as.numeric(x$runtime), dim(x$res$mast)[1]) }
```

### Update the Build bench list

```{r}
func_list <- list(run = MASTcpm_run,
                  pv = MASTcpm_pv,
                  apv = MASTcpm_apv,
                  lfc = MASTcpm_lfc,
                  time = MASTcpm_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "MAST_CPM_LRT",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "MAST",
                                                                  pkg_vers = as.character(packageVersion("MAST"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             BPPARAM = bpparam)
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

## Monocle

```{r}
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_monocle.R
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_monoclecount.R
monocle_run <- function(countData, group) {
    
    tictoc::tic()
    
    dge <- edgeR::DGEList(counts = countData)
    dge <- edgeR::calcNormFactors(dge, method = "TMM")
    lgcpms <- edgeR::cpm(dge, log = T)
    
    # # cpms <- edgeR::cpm(countData)
    # mon <- monocle::newCellDataSet(cpms,#as.matrix(L$tpm), 
    #                                phenoData = new("AnnotatedDataFrame", 
    #                                                data = data.frame(condition = group, 
    #                                                                  row.names = colnames(countData))),
    #                                expressionFamily = VGAM::tobit())
    mon <- monocle::newCellDataSet(as.matrix(countData), 
                          phenoData = new("AnnotatedDataFrame", 
                                          data = data.frame(condition = group, 
                                                            row.names = colnames(countData))),
                          lowerDetectionLimit = 0.5,
                          expressionFamily = VGAM::negbinomial())
    mon <- BiocGenerics::estimateSizeFactors(mon)
    mon <- BiocGenerics::estimateDispersions(mon)
    monres <- monocle::differentialGeneTest(mon, fullModelFormulaStr = " ~ condition")
    
    lfc <- rowMeans(lgcpms[, group[group == levels(group)[1]]]) - 
        rowMeans(lgcpms[, group[group == levels(group)[2]]])
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = monres, 
         lfc = lfc,
         runtime = runtime$toc - runtime$tic)
}

monocle_pv <- function(x) {
    res <- x$res$pval
    res[is.na(res)] <- 1
    res
}

monocle_apv <- function(x) {
    res <- x$res$qval
    res[is.na(res)] <- 1
    res
}

monocle_lfc <- function(x) {
    x$lfc
}

monocle_time <- function(x) { rep(as.numeric(x$runtime), length(x$res$pval)) }
```

### Update the Build bench list

```{r}
func_list <- list(run = monocle_run,
                  pv = monocle_pv,
                  apv = monocle_apv,
                  lfc = monocle_lfc,
                  time = monocle_time)

sbL <-
    bplapply(sbL,
             function(sb,
                      func_list) {
                 bd <- SummarizedBenchmark::BenchDesign(sb)
                 
                 bd <- SummarizedBenchmark::addMethod(bd = bd,
                                                      label = "monocle_Dt_LRT",
                                                      func = func_list$run,
                                                      post = list(pv = func_list$pv,
                                                                  adj_pv = func_list$apv,
                                                                  lfc = func_list$lfc,
                                                                  runtime = func_list$time),
                                                      meta = list(pkg_name = "monocle",
                                                                  pkg_vers = as.character(packageVersion("monocle"))),
                                                      params = rlang::quos(countData = cntdat,
                                                                           group = coldat$condition))
                 
                 SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
             },
             func_list = func_list,
             BPPARAM = bpparam)
```

```{r}
# error handling
show_dt <-
  dcast(melt(rbindlist(lapply(sbL,
                              function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
                                                    keep.rownames = "Assay")),
                       idcol = "DS"),
             id.vars = c("DS", "Assay"),
             variable.name = "Method"),
        formula = DS + Method ~ Assay)

show_dt$DS <- factor(show_dt$DS)
show_dt$Method <- factor(show_dt$Method)

datatable(show_dt[adj_pv != "success" |
                    lfc != "success" |
                    pv != "success" |
                    runtime != "success"],
          caption = "Methods that failed",
          filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
qsave(x = sbL,
      file = sumBench_perf_metrics_qs,
      nthreads = multicoreWorkers(),
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  


## BPSC

```{r}
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_BPSC.R

BPSC_run <- function(countData, group) {
  
    tictoc::tic()
    
    cpms <- edgeR::cpm(countData, 
                       lib.size = colSums(countData) * edgeR::calcNormFactors(countData))
    controlIds <- which(group == levels(factor(group))[1])
    design <- model.matrix(~ group)
    coef <- 2
    resbp <- BPSC::BPglm(data = cpms, 
                         controlIds = controlIds,
                         design = design, 
                         coef = coef, 
                         estIntPar = FALSE)
    
    
    runtime <- tictoc::toc(log = F, quiet = T)
    
    list(res = resbp, 
         runtime = runtime$toc - runtime$tic)
}

BPSC_pv <- function(x) {
    res <- x$res$PVAL
    res[is.na(res)] <- 1
    res
}

BPSC_apv <- function(x) {
    res <- p.adjust(x$res$PVAL, method = "BH")
    res[is.na(res)] <- 1
    res
}

BPSC_lfc <- function(x) {
    ##TODO
    # x$res$avg_log2FC
    x
}

BPSC_time <- function(x) { rep(as.numeric(x$runtime), length(x$res$PVAL)) }
```

### Update the Build bench list

NOT RUN 

```{r}
# func_list <- list(run = BPSC_run,
#                   pv = BPSC_pv,
#                   apv = BPSC_apv,
#                   lfc = BPSC_lfc,
#                   time = BPSC_time)

# sbL <-
#     bplapply(sbL,
#              function(sb,
#                       func_list) {
#                  bd <- SummarizedBenchmark::BenchDesign(sb)
#                  
#                  bd <- SummarizedBenchmark::addMethod(bd = bd,
#                                                       label = "BPSC_Dt_LRT",
#                                                       func = func_list$run,
#                                                       post = list(pv = func_list$pv,
#                                                                   adj_pv = func_list$apv,
#                                                                   lfc = func_list$lfc,
#                                                                   runtime = func_list$time),
#                                                       meta = list(pkg_name = "BPSC",
#                                                                   pkg_vers = as.character(packageVersion("BPSC"))),
#                                                       params = rlang::quos(countData = cntdat,
#                                                                            group = coldat$condition))
#                  
#                  SummarizedBenchmark::updateBench(sb = sb, bd = bd, dryrun = F)
#              },
#              func_list = func_list,
#              BPPARAM = bpparam)
```

```{r}
# # error handling
# show_dt <-
#   dcast(melt(rbindlist(lapply(sbL,
#                               function(x)data.table(as.data.frame(simplify2array(metadata(x)$sessions[[1]]$results)),
#                                                     keep.rownames = "Assay")),
#                        idcol = "DS"),
#              id.vars = c("DS", "Assay"),
#              variable.name = "Method"),
#         formula = DS + Method ~ Assay)
# 
# show_dt$DS <- factor(show_dt$DS)
# show_dt$Method <- factor(show_dt$Method)
# 
# datatable(show_dt[adj_pv != "success" |
#                     lfc != "success" |
#                     pv != "success" |
#                     runtime != "success"],
#           caption = "Methods that failed",
#           filter = "top", rownames = F)
```

```{r}
# save the updated summarizedBenchmark
# sumBench_perf_metrics_qs <- file.path(outdir, "updated_sumBenchs.qs")
# qsave(x = sbL,
#       file = sumBench_perf_metrics_qs,
#       nthreads = multicoreWorkers(),
#       preset = "fast")


# The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>. 
```

## Wilcoxon

```{r}
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_Wilcoxon.R
# suppressPackageStartupMessages(library(edgeR))
# 
wilcoxon_run <- function(countData, group) {
    
    tictoc::tic()
    
    dge <- edgeR::calcNormFactors(countData, method = "TMM")
    cpms <- edgeR::cpm(dge)
    idx <- 1:nrow(cpms)
    names(idx) <- rownames(cpms)
    wilcox_p <- sapply(idx, function(i) {
        wilcox.test(countData[i, ] ~ group)$p.value
    })
    
    hist(wilcox_p, 50)

    list(session_info = session_info,
         timing = timing,
         df = data.frame(pval = wilcox_p,
                         row.names = names(wilcox_p)))
}
```

## SCDE

```{r}
## SCDE
# https://github.com/csoneson/conquer_comparison/blob/master/scripts/apply_SCDE.R
# suppressPackageStartupMessages(library(scde))
# 
run_SCDE <- function(countData, group) {
    
    tictoc::tic()
    
    intcount <- apply(countData, 2, 
                      function(x) {
                          storage.mode(x) <- 'integer'
                          x
                      }
    )
    
    o.ifm <- scde::scde.error.models(counts = intcount, 
                                     groups = group, 
                                     linear.fit = T,
                                     min.count.threshold = 1,
                                     n.cores = 1,
                                     threshold.segmentation = TRUE,
                                     save.crossfit.plots = FALSE, 
                                     save.model.plots = FALSE,
                                     verbose = 0, 
                                     min.size.entries = min(2000, nrow(countData) - 1))
    
    valid.cells <- o.ifm$corr.a > 0
    # table(valid.cells)
    o.ifm <- o.ifm[valid.cells, ]
    o.prior <- scde::scde.expression.prior(models = o.ifm, 
                                     counts = intcount[, valid.cells],
                                     length.out = 400, 
                                     show.plot = FALSE)
    grp <- factor(group[which(valid.cells)])
    names(grp) <- rownames(o.ifm)
    ediff <- scde::scde.expression.difference(o.ifm, 
                                        intcount[, valid.cells], 
                                        o.prior,
                                        groups = grp, n.randomizations = 100,
                                        n.cores = 1, 
                                        verbose = 0)
    p.values <- 2 * pnorm(abs(ediff$Z), lower.tail = FALSE)
    p.values.adj <- 2 * pnorm(abs(ediff$cZ), lower.tail = FALSE)

  hist(p.values, 50)
  hist(p.values.adj, 50)

  list(session_info = session_info,
       timing = timing,
       res = ediff,
       df = data.frame(pval = p.values,
                       padj = p.values.adj,
                       score = abs(ediff$Z),
                       row.names = rownames(ediff)))
}
```

## baySeq

```{r}
# baySeq <- function(countData, group, design) {
#     tictoc::tic()
#     baySeq.cd <- new('countData',
#                      data = countData,
#                      replicates = group,
#                      groups = list(NDE = rep(1, length(design)),
#                                    DE = design))
#     
#     libsizes(baySeq.cd) <- baySeq::getLibsizes(baySeq.cd, estimationType = "edgeR")
#     
#     baySeq.cd <- baySeq::getPriors.NB(baySeq.cd,
#                                       # samplesize = sample.size,
#                                       equalDispersions = TRUE,
#                                       estimation = "QL",
#                                       cl = NULL)
#     baySeq.cd <- baySeq::getLikelihoods.NB(baySeq.cd,
#                                            # prs = c(0.5, 0.5), 
#                                            pET = "BIC", 
#                                            cl = NULL)
#     baySeq.cd@annotation <- data.frame(rowID = rownames(baySeq.cd@data),
#                                        row.names = rownames(baySeq.cd@data))
#     baySeq.posteriors.DE <- exp(baySeq.cd@posteriors)[, 'DE']
#     baySeq.FDR <- baySeq::topCounts(baySeq.cd, group = 'DE',
#                                     FDR = 1)$FDR.DE[match(rownames(countData),
#                                                           rownames(baySeq::topCounts(baySeq.cd,
#                                                                                      group = 'DE', FDR = 1)))]
#     baySeq.score <- 1 - baySeq.FDR
#     data.frame('FDR' = baySeq.FDR, 'score' = baySeq.score, 'posterior.DE' = baySeq.posteriors.DE)
# }
```

## ShrinkBayes

```{r}
# shrinkBayes_run <- 
#     function(countData, groups) {
#       
#       ## TODO: complete this function
#         
#       tictoc::tic()
#       
#       ## https://github.com/markvdwiel/ShrinkBayes/blob/master/inst/doc/ShrinkBayes.pdf
#       # library(edgeR)
#       DGE <- edgeR::DGEList(countData, group = groups)
#       libsize <- colSums(countData, na.rm = T)
#       DGEnorm <- edgeR::calcNormFactors(DGE, method = "TMMwsp")
#       normfac0 <- DGEnorm$samples[,3]
#       rellibsize <- libsize / exp(mean(log(libsize)))
#       normfac <- normfac0 * rellibsize
#       myoffsets <- log(normfac)
#       form <- ~ 1 + groups + offset(myoffsets)
#       # form0 <- ~ 1 + offset(myoffsets)
#       
#       library(INLA)
#       library(ShrinkBayes)
#       SBmir <- ShrinkBayes::ShrinkBayesWrap(countData, form, ncpus = 1)
#       # shrinksimul <- ShrinkSeq(form = form,
#       #                          dat = countData,
#       #                          shrinkfixed = "g",
#       #                          fams = "zinb")
#       
#       # form0 <- y ~ 1
#       # shrinksimul <- ShrinkSeq(form = form, 
#       #                          dat = physeq@otu_table@.Data, 
#       #                          shrinkfixed = "g", 
#       #                          fams = "zinb")
#       # fitall <- FitAllShrink(form, 
#       #                        dat = norm.counts, 
#       #                        fams = "zinb",
#       #                        shrinksimul = shrinksimul)
#       # fitall0 <- FitAllShrink(form0, 
#       #                         dat = norm.counts, 
#       #                         fams = "zinb",
#       #                         shrinksimul = shrinksimul)  
#       # npprior <- MixtureUpdatePrior(fitall = fitall, 
#       #                               fitall0 = fitall0, 
#       #                               shrinkpara="g", 
#       #                               ncpus = mc.cores)
#       # nppostshr <- MixtureUpdatePosterior(fitall, npprior, fitall0)
#       # lfdr <- SummaryWrap(nppostshr)
#       # fdr <- BFDR(lfdr)
#       # pGlobal <- as.vector(fdr)
#       # rm(list = c("ptm", "g", "form", "form0", "shrinksimul", 
#       #             "fitall", "fitall0", "npprior" ,"nppostshr", "lfdr", "fdr"))
#       # 
#       # pValMat <- as.matrix(pGlobal)
#       # colnames(pValMat) <- c("adjP")
#       # statInfo <- NULL
#       
#       res <- list("pValMat" = pValMat, "statInfo" = statInfo)
#       # res <- data.frame(p.value = pval, 
#       #               logFC = lfc)
#       
#       runtime <- tictoc::toc(log = F, quiet = T)
#       
#       list(res = res, 
#            runtime = runtime$toc - runtime$tic)
#     }
# 
# #TODO
# shrinkBayes_pv <- function(x) {}
# 
# shrinkBayes_apv <- function(x) {}
# 
# shrinkBayes_lfc <- function(x) {}
# 
# shrinkBayes_time <- function(x) { x$runtime }
```

## GMPR + {DESeq2, edgeR, ....}

```{r}
# https://peerj.com/articles/4600/
# GMPR: A robust normalization method for zero-inflated count data with application to microbiome sequencing data
# https://github.com/jchen1981/GMPR
# # Demonstrate the use of GMPR factor
# #setwd('path2GMPR')
# 
# require(GUniFrac)
# require(vegan)
# require(DESeq2)
# source('GMPR.R')
# 
# data(throat.otu.tab)
# data(throat.meta)
# 
# #########################################
# # Calculate GMPR size factor
# # Row - features, column - samples
# otu.tab <- t(throat.otu.tab)
# gmpr.size.factor <- GMPR(otu.tab)
# # Application 2:  Differential abundance analysis using DESeq2 using GMPR size factors instead of the default
# dds <- DESeqDataSetFromMatrix(countData = otu.tab,
# 		colData = throat.meta,
# 		design= ~ SmokingStatus)
# 
# # Replace size factor
# # dds <- estimateSizeFactors(dds)
# sizeFactors(dds) <-  gmpr.size.factor
# dds <- estimateDispersions(dds)
# dds <- nbinomWaldTest(dds)
# results(dds, contrast=c("SmokingStatus", "NonSmoker", "Smoker"))
```

# Session info

```{r}
sessionInfo()
```

