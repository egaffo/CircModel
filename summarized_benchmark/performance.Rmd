---
title: "Compute performance of built SummarizedBenchmark objects"
author: "Enrico Gaffo"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    code_folding: hide
params:
  outdir: "DM1_performance"
  inputData: "./DM1_updated_benchmark_results/updated_sumBenchs.qs"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(SummarizedBenchmark)
library("magrittr")
# library(plotROC)
library(data.table)
library(qs)
library(DT)

# library("limma")
# library("edgeR")
# library("DESeq2")
# library("tximport")
# library(glmGamPoi)
# library(zinbwave)
# library(scran)

library(BiocParallel)
library(batchtools)

# install.packages("devtools", Ncpus = multicoreWorkers())
# library(devtools)
# install_github("lichen-lab/circMeta", Ncpus = multicoreWorkers())
# library(circMeta)
# library(lncDIFF)
# library(ShrinkBayes)
# library(samr)
```

```{r}
## set the output directory
outdir <- params$outdir
dir.create(path = outdir, recursive = T, showWarnings = F)
```

```{r}
nWorkers <- multicoreWorkers()

## load preprocessed benchmark data
inputData <- params$inputData
sbL <- qread(file = inputData, nthreads = nWorkers)
```

# Performance

```{r}
# availableMetrics()
```

## Add performance 

```{r}
#' Adds performance metrics to a SummarizedBenchmark object
#' param x the SummarizedBenchmark object
#' returns a SummarizedBenchmark with the peformance metrics setted
add_performance_metrics <- 
  function(x) {
    
    ## Notes:
    ## precision (PPV) = 1 - FDR
    ## recall = TPR
    ## specificity = TNR  
    ## FPR = 1 - TNR
    ## FNR = 1 - TPR
    ## F1 = 2 * ( (PPV * TPR) / (PPV + TPR) )
    
    ## add the metrics on the P-values
    x <- SummarizedBenchmark::addPerformanceMetric(x, 
                                                   evalMetric = c("rejections", "TPR", "TNR", "FDR"), #, "FNR"
                                                   assay = "pv")
    
    ## add the metrics on the adjusted P-values
    x <- SummarizedBenchmark::addPerformanceMetric(x, 
                                                   evalMetric = c("rejections", "TPR", "TNR", "FDR"), #, "FNR"
                                                   assay = "adj_pv")
    
    ## add the metrics on the fold changes: TPR
    x <- SummarizedBenchmark::addPerformanceMetric(object = x,
                                                   assay = "lfc",
                                                   evalMetric = "LFC_TPR",
                                                   evalFunction = function(query, truth, lfc_thr = 0.5) {
                                                     ## TPR = TP / (TP + FN)
                                                     is_lfc_larger  <- query >= lfc_thr
                                                     is_lfc_larger[is.na(is_lfc_larger)] <- F
                                                     TP <- sum(is_lfc_larger & truth == 1)
                                                     TP / sum(truth == 1)
                                                   })
    
    ## add the metrics on the fold changes: TNR
    x <- SummarizedBenchmark::addPerformanceMetric(object = x,
                                                   assay = "lfc",
                                                   evalMetric = "LFC_TNR", 
                                                   evalFunction = function(query, truth, lfc_thr = 0.5) {
                                                     ## TNR = TN / N, with N = TN + FP
                                                     is_lfc_lower  <- query < lfc_thr
                                                     is_lfc_lower[is.na(is_lfc_lower)] <- T
                                                     TN <- sum(is_lfc_lower & truth == 0)
                                                     TN / sum(truth == 0)
                                                   })
    
    ## add the metrics on the fold changes: FDR
    x <- SummarizedBenchmark::addPerformanceMetric(object = x,
                                                   assay = "lfc",
                                                   evalMetric = "LFC_FDR", 
                                                   evalFunction = function(query, truth, lfc_thr = 0.5) {
                                                     ## FDR = FP / (FP + TP)
                                                     is_lfc_larger  <- query >= lfc_thr
                                                     is_lfc_larger[is.na(is_lfc_larger)] <- F
                                                     FP <- sum(is_lfc_larger & truth == 0)
                                                     TP <- sum(is_lfc_larger & truth == 1)
                                                     FP / (FP + TP)
                                                   })
    
    ## add the Runtime metric
    x <- SummarizedBenchmark::addPerformanceMetric(object = x,
                                                   assay = "runtime",
                                                   evalMetric = "Runtime",
                                                   evalFunction = function(query, truth, add_weight_time = FALSE) {
                                                     ifelse(add_weight_time, query[1] + truth[1], query[1])
                                                   })
    
    ## return the updated SummarizedBenchmark object
    x
  }
```

```{r add_performance_metrics}
## add the performance metrics to the list of bechDesign
bpparam <- BatchtoolsParam(workers = length(sbL), 
                           saveregistry = F,
                           cluster = "slurm",
                           resources = list(ncpus = 1, 
                                            walltime = 600, # 10min
                                            memory = 2048) # 4Gb, 2048 2GByte, 8192 8Gb
)

sbL <- 
  bplapply(sbL, 
           add_performance_metrics,
           # BPPARAM = bpparam)
           BPPARAM = BiocParallel::MulticoreParam(min(4, nWorkers)))
```

## Estimate performance

```{r estimate_performance}
alpha_targets <- c("0.01" = 0.01, "0.05" = 0.05, "0.1" = 0.1) 
add_weights <- c(FALSE, TRUE)

# sbL <- bplapply(sbL, function(x, alphas, add_weights) {
#     SummarizedBenchmark::estimatePerformanceMetrics(x, 
#                                                     rerun = T,
#                                                     alpha = alphas, 
#                                                     add_weight_time = add_weights,
#                                                     addColData = T)},
#     alphas = alpha_targets,
#     add_weights = add_weights,
#     # BPPARAM = BiocParallel::SerialParam())
#     BPPARAM = BiocParallel::MulticoreParam(2))
# # BPPARAM = bpparam)

sbL <- lapply(sbL, function(x, alphas, add_weights) {
    SummarizedBenchmark::estimatePerformanceMetrics(x, 
                                                    rerun = T,
                                                    alpha = alphas, 
                                                    add_weight_time = add_weights,
                                                    addColData = T)},
    alphas = alpha_targets,
    add_weights = add_weights)

# View(rbindlist(lapply(sbL,
#                       function(x)data.table(as.data.frame(colData(x)),
#                                             keep.rownames = "Met")),
#                idcol = "DS"))
```

```{r save_performance_results}
## save the summarizedBenchmark estimated performance
sumBench_perf_metrics_qs <- file.path(outdir, "sumBench_perf_metrics.qs")
qsave(x = sbL, 
      file = sumBench_perf_metrics_qs, 
      nthreads = multicoreWorkers(), 
      preset = "fast")
```

The benchmark results have been save into <a href="`r sumBench_perf_metrics_qs`">`r sumBench_perf_metrics_qs`</a>.  

# Session info

```{r}
sessionInfo()
```

